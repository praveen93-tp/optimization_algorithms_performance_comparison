# optimization_algorithms_performance_comparison
Datasets:

https://www.kaggle.com/vikramtiwari/mnist-numpy

https://www.cs.toronto.edu/~kriz/cifar.html

1) Adam (learning_rate_decay+weight_decay+l2)
2) RAdam (learning_rate_decay+weight_decay+l2)
3) Lookahead Optimizer + Adam
4) Lookahead Optimizer + Sgd
5) AdamW
6) Amsgrad
7) Adabound
8) NAdam
9) Adamax
10) PAdam
